{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/6/6d/Nvidia_image_logo.svg/1200px-Nvidia_image_logo.svg.png\" width=\"250\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2020 NVIDIA\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection w/ TensorRT\n",
    "\n",
    "This notebook will illustrate the full pipeline of taking a TensorFlow model for object detection (in this case an SSD model), creating a TensorRT engine, and running inference.\n",
    "\n",
    "These are the type of results that we are trying to achieve while working through this notebook:\n",
    "\n",
    "|   Framework for Inference  | Inference Time (ms) | Frames Per Second (fps) | Speedup |\n",
    "|----------------------------|:-------------------:|:-----------------------:|:-------:|\n",
    "| TensorFlow                 |         276         |          3.6            |    1x   |            \n",
    "| TensorRT (FP32)            |          48         |          20.8           |   5.8x  |\n",
    "| TensorRT (FP16)            |          38         |          26.3           |   7.3x  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting MaxN mode\n",
    "\n",
    "First, before we do anything else, we want to make sure that we put the Jetson Nano in MaxN mode.  This will mean that the the clock frequencies will be set to their highest in order to achieve the lowest inference times.\n",
    "\n",
    "To do this, we want to set the max/min frequency to a preferred value, which we can do with the `nvpmodel` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo nvidia | sudo -S nvpmodel -m 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo nvidia | sudo -S nvpmodel -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to use the `jetson_clocks` script to fix the frequency to maximal.  This will allow us to get the most performance out of our device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo nvidia | sudo -S jetson_clocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Architecture\n",
    "\n",
    "As you may be aware, there are numerous deep learning architectures geared towards object detection (or other computer vision tasks like segmentation).  Some of these networks are designed to be as performant as possible in terms of accuracy while others are designed for speed.  Since we will be focusing on object detection applications for embedded devices (namely, Jetson Nano), we want to choose a network that has been designed for speed and will still be performant enough on an embedded device.  For this reason we have chosen the Single-Shot Multibox Detector (SSD); one could also choose networks like YOLO that are also single-shot pipelines.  Other architectures (Faster RCNN, for example) contain a two-stage pipeline where region proposals are created during one stage of the pipeline and then the second stage evaluates each of these proposals.  For SSD and YOLO, this is all done in a single pipeline.  To learn more about the Single Shot Detector (SSD) architecture, you can read the [paper](https://arxiv.org/abs/1512.02325).  The network architecture is shown below:\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/fit/t/1600/480/1*hdSE1UCV7gA7jzfQ03EnWw.png\" alt=\"SSD Architecture\" width=\"1000\"/></center>\n",
    "<center>Image credit: https://cdn-images-1.medium.com/fit/t/1600/480/1*hdSE1UCV7gA7jzfQ03EnWw.png</center>\n",
    "\n",
    "For this lab, we will be using the Single-Shot MultiBox Detector (SSD) architecture as our starting point.  The above image shows a backbone network (feature extractor) of VGG16.  We will instead use an Mobilenet V2 backbone that will help in classifying objects a little better with a slightly better performance for an embedded device.\n",
    "\n",
    "Let's get started..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's go ahead and import all the necessary packages that we will use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorrt as trt\n",
    "import graphsurgeon as gs\n",
    "import uff\n",
    "\n",
    "import os\n",
    "import ctypes\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import Image as read_img\n",
    "from IPython.display import display as disp_img\n",
    "\n",
    "import utils.inference as inference_utils\n",
    "import utils.model as model_utils\n",
    "import utils.boxes as boxes_utils\n",
    "import utils.coco as coco_utils\n",
    "import utils.engine as engine_utils\n",
    "from utils.paths import PATHS\n",
    "import common\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('TensorRT version:', trt.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also setup a few variables that will make our life easier later in the notebook.  Some of the important ones include:\n",
    "\n",
    "- Paths to models, intermediate file formats (uff), and TensorRT engines\n",
    "- TensorRT parameters (precision, batch_size, etc.)\n",
    "- Visualization information (thresholding, images, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO label list\n",
    "COCO_LABELS = coco_utils.COCO_CLASSES_LIST\n",
    "\n",
    "USER = os.environ.get('USER')\n",
    "\n",
    "# Confidence threshold for drawing bounding box\n",
    "VISUALIZATION_THRESHOLD = 0.5\n",
    "\n",
    "# Precision command line argument -> TRT Engine datatype\n",
    "TRT_PRECISION_TO_DATATYPE = {\n",
    "    16: trt.DataType.HALF,\n",
    "    32: trt.DataType.FLOAT\n",
    "}\n",
    "\n",
    "# Layout of TensorRT network output metadata\n",
    "TRT_PREDICTION_LAYOUT = {\n",
    "    \"image_id\": 0,\n",
    "    \"label\": 1,\n",
    "    \"confidence\": 2,\n",
    "    \"xmin\": 3,\n",
    "    \"ymin\": 4,\n",
    "    \"xmax\": 5,\n",
    "    \"ymax\": 6\n",
    "}\n",
    "\n",
    "# Define model variables\n",
    "PRECISION = 16\n",
    "os.environ[\"PRECISION\"] = str(PRECISION)\n",
    "MAX_BATCH_SIZE, BATCH_SIZE = 1, 1\n",
    "\n",
    "# Define path variables for all models\n",
    "#MODEL_NAME = 'ssd_inception_v2_coco_2017_11_17'\n",
    "MODEL_NAME = 'ssd_mobilenet_v2_coco_2018_03_29'\n",
    "MODEL_PATH = os.path.join('/home', USER, 'jetson-training/models')\n",
    "ENGINE_PATH = os.path.join('/home', USER, 'jetson-training/engines')\n",
    "TF_FROZEN_MODEL_PATH = os.path.join(MODEL_PATH, MODEL_NAME, 'frozen_inference_graph.pb')\n",
    "SAVED_MODEL_PATH = os.path.join(MODEL_PATH, MODEL_NAME, 'saved_model')\n",
    "UFF_MODEL_PATH = os.path.join(MODEL_PATH, MODEL_NAME, 'froze_inference_graph.uff')\n",
    "TRT_ENGINE_PATH = os.path.join(ENGINE_PATH, 'ssd_mobilenet_v2_fp16.engine') if PRECISION == 16 else os.path.join(ENGINE_PATH, 'ssd_mobilenet_v2_fp32.engine')\n",
    "TFTRT_MODEL_PATH = os.path.join(MODEL_PATH, MODEL_NAME, 'ssd_mobilenet_v2_tftrt_fp16') if PRECISION == 16 else os.path.join(MODEL_PATH, MODEL_NAME, 'ssd_mobilenet_v2_tftrt_fp32')\n",
    "\n",
    "# Define data for testing the models\n",
    "IMAGE_PATH = os.path.join('/home', USER, 'jetson-training/images')\n",
    "INPUT_IMG = os.path.join(IMAGE_PATH, 'dogs.jpg')\n",
    "OUTPUT_IMG = os.path.join(IMAGE_PATH, 'ssd_output.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TensorFlow model path: {}'.format(TF_FROZEN_MODEL_PATH))\n",
    "print('UFF model path: {}'.format(UFF_MODEL_PATH))\n",
    "print('TensorRT engine path: {}'.format(TRT_ENGINE_PATH))\n",
    "\n",
    "print('\\nUsing precision mode {} ({}-bit float) for building engine and inference'.format(TRT_PRECISION_TO_DATATYPE[PRECISION], PRECISION))\n",
    "\n",
    "print('\\nInput image for inference')\n",
    "disp_img(read_img(filename=INPUT_IMG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that we have everything setup, we can start with benchmarking the TensorFlow model.\n",
    "\n",
    "### TensorFlow Inference\n",
    "\n",
    "First we want to use our original model that was taken from the [Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) [Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).\n",
    "\n",
    "This will allow us to have a baseline to compare our TensorRT optimized model for fp32 and fp16 once we generate them.\n",
    "\n",
    "#### Helper Functions\n",
    "\n",
    "These functions provide us the ability to load an image and then convert that image into a numpy array (which will then be processed by the network).\n",
    "\n",
    "First we can define a little bit about the model so that we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelData(object):\n",
    "    # Name of input node\n",
    "    INPUT_NAME = \"Input\"\n",
    "    # CHW format of model input\n",
    "    INPUT_SHAPE = (3, 300, 300)\n",
    "    # Name of output node\n",
    "    OUTPUT_NAME = \"NMS\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_input_channels():\n",
    "        return ModelData.INPUT_SHAPE[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_input_height():\n",
    "        return ModelData.INPUT_SHAPE[1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_input_width():\n",
    "        return ModelData.INPUT_SHAPE[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define a couple of helper functions that will allow us to load an image and convert it to a numpy array easily for inference with TensorFlow and TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image).reshape(\n",
    "        (im_height, im_width, ModelData.get_input_channels())\n",
    "    ).astype(np.uint8)\n",
    "\n",
    "def load_img(image_path, fw='tf'):\n",
    "    \"\"\"Infers model on given image.\n",
    "    Args:\n",
    "         image_path (str): image to run classification/object detection model on\n",
    "         fw (str): either 'tf' for tensorflow or 'trt' for tensorrt\n",
    "    \"\"\"\n",
    "    # Load image into CPU\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Get model input width and height\n",
    "    model_input_width = ModelData.get_input_width()\n",
    "    model_input_height = ModelData.get_input_height()\n",
    "    # Note: Bilinear interpolation used by Pillow is a little bit\n",
    "    # different than the one used by Tensorflow, so if network receives\n",
    "    # an image that is not 300x300, the network output may differ\n",
    "    # from the one output by Tensorflow\n",
    "    image_resized = image.resize(\n",
    "        size=(model_input_width, model_input_height),\n",
    "        resample=Image.BILINEAR\n",
    "    )\n",
    "    \n",
    "    img_np = load_image_into_numpy_array(image_resized)\n",
    "    if fw == 'tf':\n",
    "        img_np = np.expand_dims(img_np, axis=0)\n",
    "    else:\n",
    "        img_np = img_np.transpose((2, 0, 1))\n",
    "        # Normalize to [-1.0, 1.0] interval (expected by model)\n",
    "        img_np = (2.0 / 255.0) * img_np - 1.0\n",
    "        img_np = img_np.ravel()\n",
    "    \n",
    "    return img_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download model that we will use for the rest of this notebook.  In this case, we will use the SSD Mobilenet V2 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(MODEL_PATH, MODEL_NAME)):\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.mkdir(MODEL_PATH)\n",
    "    print(\"Preparing pre-trained SSD model\")\n",
    "    model_dir = os.path.join(MODEL_PATH, MODEL_NAME)\n",
    "    model_url = PATHS.get_model_url(MODEL_NAME)\n",
    "    model_archive_path = os.path.join(MODEL_PATH, \"{}.tar.gz\".format(MODEL_NAME))\n",
    "    model_utils.download_file(model_url, model_archive_path, False)\n",
    "    print(\"Download complete\\nUnpacking {}\".format(model_archive_path))\n",
    "    with tarfile.open(model_archive_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=os.path.join(MODEL_PATH))\n",
    "    print(\"Extracting complete\\nRemoving {}\".format(model_archive_path))\n",
    "    os.remove(model_archive_path)\n",
    "    print(\"Model ready\")\n",
    "else:\n",
    "    print(\"Path already exists, no need to re-download and prepare the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to setup a TensorFlow session that will:\n",
    "\n",
    "* Create input/output tensors\n",
    "* Load the input image (and convert to numpy array)\n",
    "* Start an inference timer\n",
    "* Run inference using our TensorFlow model\n",
    "\n",
    "The benchmarking numbers that are achieved from this inference execution will be used for our comparison later to TensorRT results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tf_sess():\n",
    "    times = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        with tf.gfile.GFile(TF_FROZEN_MODEL_PATH, 'rb') as f:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            _ = tf.import_graph_def(graph_def)\n",
    "            image_tensor = sess.graph.get_tensor_by_name('import/image_tensor:0')\n",
    "            boxes = sess.graph.get_tensor_by_name('import/detection_boxes:0')\n",
    "            scores = sess.graph.get_tensor_by_name('import/detection_scores:0')\n",
    "            classes = sess.graph.get_tensor_by_name('import/detection_classes:0')\n",
    "            num_detections = sess.graph.get_tensor_by_name('import/num_detections:0')\n",
    "\n",
    "            iterations = 10\n",
    "\n",
    "            input_image = load_img(INPUT_IMG, 'tf')  \n",
    "            for index in range((iterations)):\n",
    "                start = time.time()\n",
    "                sess.run([boxes, scores, classes, num_detections],feed_dict={image_tensor: input_image})\n",
    "                stop = time.time()\n",
    "                if index > 1:\n",
    "                   times.append(stop - start)\n",
    "                   print(\"Time per run for Tensorflow Inference %d: %f ms\" % (index, (stop - start)*1000))\n",
    "\n",
    "run_tf_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our baseline numbers, we want to follow the same process we followed in the first image classification example.\n",
    "\n",
    "## UFF Model Creation\n",
    "\n",
    "We want to first create a UFF model (intermediate format) and then use that intermediate format to create a TensorRT engine we can then use for inference.\n",
    "\n",
    "There are a few specialty layers in the SSD model which we have to take care of first (i.e. they are not natively supported by UFF).  Namely, the NMS layer at the end of the network for classification and the FlattenConcat layer.  These unsupported nodes will be set as \"placeholder\" nodes in the UFF graph and then replaced in the TensorRT graph with optimized nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_unsupported_nodes_to_plugin_nodes(ssd_graph):\n",
    "    \"\"\"Makes ssd_graph TensorRT comparible using graphsurgeon.\n",
    "\n",
    "    This function takes ssd_graph, which contains graphsurgeon\n",
    "    DynamicGraph data structure. This structure describes frozen Tensorflow\n",
    "    graph, that can be modified using graphsurgeon (by deleting, adding,\n",
    "    replacing certain nodes). The graph is modified by removing\n",
    "    Tensorflow operations that are not supported by TensorRT's UffParser\n",
    "    and replacing them with custom layer plugin nodes.\n",
    "\n",
    "    Note: This specific implementation works only for\n",
    "    ssd_mobilenet_v2_coco_2018_03_29 network.\n",
    "\n",
    "    Args:\n",
    "        ssd_graph (gs.DynamicGraph): graph to convert\n",
    "    Returns:\n",
    "        gs.DynamicGraph: UffParser compatible SSD graph\n",
    "    \"\"\"\n",
    "    # Create TRT plugin nodes to replace unsupported ops in Tensorflow graph\n",
    "    channels = ModelData.get_input_channels()\n",
    "    height = ModelData.get_input_height()\n",
    "    width = ModelData.get_input_width()\n",
    "\n",
    "    Input = gs.create_plugin_node(name=\"Input\",\n",
    "        op=\"Placeholder\",\n",
    "        dtype=tf.float32,\n",
    "        shape=[1, channels, height, width])\n",
    "\n",
    "    PriorBox = gs.create_plugin_node(name=\"MultipleGridAnchorGenerator\", op=\"GridAnchor_TRT\",\n",
    "        minSize=0.2,\n",
    "        maxSize=0.95,\n",
    "        aspectRatios=[1.0, 2.0, 0.5, 3.0, 0.33],\n",
    "        variance=[0.1,0.1,0.2,0.2],\n",
    "        featureMapShapes=[19, 10, 5, 3, 2, 1],\n",
    "        numLayers=6\n",
    "    )\n",
    "\n",
    "    NMS = gs.create_plugin_node(\n",
    "        name=\"NMS\",\n",
    "        op=\"NMS_TRT\",\n",
    "        shareLocation=1,\n",
    "        varianceEncodedInTarget=0,\n",
    "        backgroundLabelId=0,\n",
    "        confidenceThreshold=1e-8,\n",
    "        nmsThreshold=0.6,\n",
    "        topK=100,\n",
    "        keepTopK=100,\n",
    "        numClasses=91,\n",
    "        inputOrder=[1, 0, 2],\n",
    "        confSigmoid=1,\n",
    "        isNormalized=1\n",
    "    )\n",
    "\n",
    "    concat_priorbox = gs.create_node(\n",
    "        \"concat_priorbox\",\n",
    "        op=\"ConcatV2\",\n",
    "        dtype=tf.float32,\n",
    "        axis=2\n",
    "    )\n",
    "    \n",
    "    concat_box_loc = gs.create_plugin_node(\n",
    "        \"concat_box_loc\",\n",
    "        op=\"FlattenConcat_TRT\",\n",
    "        dtype=tf.float32,\n",
    "        axis=1,\n",
    "        ignoreBatch=0\n",
    "    )\n",
    "\n",
    "    concat_box_conf = gs.create_plugin_node(\n",
    "        \"concat_box_conf\",\n",
    "        op=\"FlattenConcat_TRT\",\n",
    "        dtype=tf.float32,\n",
    "        axis=1,\n",
    "        ignoreBatch=0\n",
    "    )\n",
    "\n",
    "    # Create a mapping of namespace names -> plugin nodes.\n",
    "    namespace_plugin_map = {\n",
    "        \"Concatenate\": concat_priorbox,\n",
    "        \"MultipleGridAnchorGenerator\": PriorBox,\n",
    "        \"Postprocessor\": NMS,\n",
    "        \"Preprocessor\": Input,\n",
    "        \"ToFloat\": Input,\n",
    "        \"image_tensor\": Input,\n",
    "        \"concat\": concat_box_loc,\n",
    "        \"concat_1\": concat_box_conf\n",
    "    }\n",
    "\n",
    "    # Create a new graph by collapsing namespaces\n",
    "    ssd_graph.collapse_namespaces(namespace_plugin_map)\n",
    "    # Remove the outputs, so we just have a single output node (NMS).\n",
    "    # If remove_exclusive_dependencies is True, the whole graph will be removed!\n",
    "    ssd_graph.remove(ssd_graph.graph_outputs, remove_exclusive_dependencies=False)\n",
    "    \n",
    "    ssd_graph.find_nodes_by_op(\"NMS_TRT\")[0].input.remove(\"Input\")\n",
    "    return ssd_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the `uff.from_tensorflow()` operator to convert our model from a frozen TensorFlow graph to a UFF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_to_uff():\n",
    "    dynamic_graph = gs.DynamicGraph(TF_FROZEN_MODEL_PATH)\n",
    "    dynamic_graph = ssd_unsupported_nodes_to_plugin_nodes(dynamic_graph)\n",
    "\n",
    "    uff.from_tensorflow(\n",
    "        dynamic_graph.as_graph_def(),\n",
    "        [ModelData.OUTPUT_NAME],\n",
    "        output_filename=UFF_MODEL_PATH,\n",
    "        text=True)\n",
    "    \n",
    "model_to_uff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT Engine Creation\n",
    "\n",
    "Now that we have our intermediate representation, let's use it to create out TensorRT engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trt_engine(trt_logger):\n",
    "    with trt.Builder(trt_logger) as builder, builder.create_network() as network, trt.UffParser() as parser:\n",
    "        builder.max_workspace_size = 1 << 30\n",
    "        if TRT_PRECISION_TO_DATATYPE[PRECISION] == trt.DataType.HALF:\n",
    "            builder.fp16_mode = True\n",
    "        builder.max_batch_size = BATCH_SIZE\n",
    "        \n",
    "        parser.register_input(ModelData.INPUT_NAME, ModelData.INPUT_SHAPE)\n",
    "        parser.register_output(\"MarkOutput_0\")\n",
    "        parser.parse(UFF_MODEL_PATH, network)\n",
    "        \n",
    "        print(\"Building TensorRT engine. This may take a few minutes.\")\n",
    "        \n",
    "        return builder.build_cuda_engine(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_engine(engine, engine_dest_path):\n",
    "    buf = engine.serialize()\n",
    "    with open(engine_dest_path, 'wb') as f:\n",
    "        f.write(buf)\n",
    "\n",
    "def load_engine(trt_runtime, engine_path):\n",
    "    with open(engine_path, 'rb') as f:\n",
    "        engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interest of time, we will not go through the engine building process, we will simply download the engine that would have been produced by the following code.\n",
    "\n",
    "If you would like to go through the engine building process, you can simply remove the `ssd_mobilenet_v2_<precision>.engine` from the `engines` directory and run the following cell.  The same engine file will be built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DOWNLOAD_PATH='/home/'${USER}'/jetson-training/engines'\n",
    "\n",
    "if [ \"$PRECISION\" == \"16\" ]; then\n",
    "    echo \"Downloading 16-bit precision version of SSD Mobilenet TensorRT engine...\"\n",
    "    FILE_ID='1Rtlvz2Ei2PpYr712IMoDjj_JjbPX03Hc'\n",
    "    ENGINE_FILE_NAME='ssd_mobilenet_v2_fp16.engine'\n",
    "elif [ \"$PRECISION\" == \"32\" ]; then\n",
    "    echo \"Downloading 32-bit precision version of SSD Mobilenet TensorRT engine...\"\n",
    "    FILE_ID='1PfObqUIPiZ7W8e0m-xInSXy3WOopgEd-'\n",
    "    ENGINE_FILE_NAME='ssd_mobilenet_v2_fp32.engine'\n",
    "fi\n",
    "\n",
    "mkdir -p ${DOWNLOAD_PATH}\n",
    "wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=${FILE_ID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=${FILE_ID}\" -O ${DOWNLOAD_PATH}/${ENGINE_FILE_NAME} && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alt engines/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "# We first load all custom plugins shipped with TensorRT,\n",
    "# some of which will be needed during inference\n",
    "trt.init_libnvinfer_plugins(TRT_LOGGER, '')\n",
    "\n",
    "# Initialize runtime needed for loading TensorRT engine from file\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "\n",
    "print(\"TensorRT inference engine settings:\")\n",
    "print(\"  * Inference precision - {}\".format(TRT_PRECISION_TO_DATATYPE[PRECISION]))\n",
    "print(\"  * Max batch size - {}\\n\".format(BATCH_SIZE))\n",
    "\n",
    "if not os.path.exists(TRT_ENGINE_PATH):\n",
    "    trt_engine = build_trt_engine(TRT_LOGGER)\n",
    "    if not os.path.exists(ENGINE_PATH):\n",
    "        os.mkdir(ENGINE_PATH)\n",
    "    save_engine(trt_engine, TRT_ENGINE_PATH)\n",
    "else:\n",
    "    print(\"TensorRT model already exists at {}\".format(TRT_ENGINE_PATH))\n",
    "    print(\"Using this engine instead of creating a new one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our TensorRT engine, let's load it, and run inference and compare it to the TensorFlow runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_engine = load_engine(trt_runtime, TRT_ENGINE_PATH)\n",
    "\n",
    "# This allocates memory for network inputs/outputs on both CPU and GPU\n",
    "inputs, outputs, bindings, stream = engine_utils.allocate_buffers(trt_engine)\n",
    "\n",
    "# Execution context is needed for inference\n",
    "context = trt_engine.create_execution_context()\n",
    "\n",
    "# Allocate memory for multiple usage [e.g. multiple batch inference]\n",
    "input_volume = trt.volume(ModelData.INPUT_SHAPE)\n",
    "numpy_array = np.zeros((trt_engine.max_batch_size, input_volume))\n",
    "\n",
    "# Load image into CPU\n",
    "img = load_img(INPUT_IMG, 'trt')\n",
    "\n",
    "# Copy it into appropriate place in memory\n",
    "# inputs was returned earlier by allocate_buffers()\n",
    "np.copyto(inputs[0].host, img.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_start_time = time.time()\n",
    "\n",
    "# Fetch output from the model\n",
    "[detection_out, keepCount_out] = common.do_inference(\n",
    "    context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this inference, you can see that you can get ~4x performance improvement by just converting your model to TensorRT.\n",
    "\n",
    "Let's make sure that the output of the network is proper.  Let's load the image and draw the output bounding boxes on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_prediction_field(field_name, detection_out, pred_start_idx):\n",
    "    \"\"\"Fetches prediction field from prediction byte array.\n",
    "\n",
    "    After TensorRT inference, prediction data is saved in\n",
    "    byte array and returned by object detection network.\n",
    "    This byte array contains several pieces of data about\n",
    "    prediction - we call one such piece a prediction field.\n",
    "    The prediction fields layout is described in TRT_PREDICTION_LAYOUT.\n",
    "\n",
    "    This function, given prediction byte array returned by network,\n",
    "    staring index of given prediction and field name of interest,\n",
    "    returns prediction field data corresponding to given arguments.\n",
    "\n",
    "    Args:\n",
    "        field_name (str): field of interest, one of keys of TRT_PREDICTION_LAYOUT\n",
    "        detection_out (array): object detection network output\n",
    "        pred_start_idx (int): start index of prediction of interest in detection_out\n",
    "\n",
    "    Returns:\n",
    "        Prediction field corresponding to given data.\n",
    "    \"\"\"\n",
    "    return detection_out[pred_start_idx + TRT_PREDICTION_LAYOUT[field_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction(detection_out, pred_start_idx, img_pil):\n",
    "    image_id = int(fetch_prediction_field(\"image_id\", detection_out, pred_start_idx))\n",
    "    label = int(fetch_prediction_field(\"label\", detection_out, pred_start_idx))\n",
    "    confidence = fetch_prediction_field(\"confidence\", detection_out, pred_start_idx)\n",
    "    xmin = fetch_prediction_field(\"xmin\", detection_out, pred_start_idx)\n",
    "    ymin = fetch_prediction_field(\"ymin\", detection_out, pred_start_idx)\n",
    "    xmax = fetch_prediction_field(\"xmax\", detection_out, pred_start_idx)\n",
    "    ymax = fetch_prediction_field(\"ymax\", detection_out, pred_start_idx)\n",
    "    if confidence > VISUALIZATION_THRESHOLD:\n",
    "        class_name = COCO_LABELS[label]\n",
    "        confidence_percentage = \"{0:.0%}\".format(confidence)\n",
    "        print(\"Detected {} with confidence {}\".format(\n",
    "            class_name, confidence_percentage))\n",
    "        boxes_utils.draw_bounding_boxes_on_image(\n",
    "            img_pil, np.array([[ymin, xmin, ymax, xmax]]),\n",
    "            display_str_list=[\"{}: {}\".format(\n",
    "                class_name, confidence_percentage)],\n",
    "            color=coco_utils.COCO_COLORS[label]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pil = Image.open(INPUT_IMG)\n",
    "prediction_fields = len(TRT_PREDICTION_LAYOUT)\n",
    "\n",
    "for det in range(int(keepCount_out[0])):\n",
    "    analyze_prediction(detection_out, det * prediction_fields, img_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for this particular image we have detected 4 objects that have a higher confidence value than 50% (the threshold we set earlier when setting out environment variables).\n",
    "\n",
    "Now let's save and view the image to finalize the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pil.save(OUTPUT_IMG)\n",
    "print(\"Saved output image to: {}\".format(OUTPUT_IMG))\n",
    "disp_img(read_img(filename=OUTPUT_IMG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the bounding boxes on the dogs are pretty good, but the two on the single person aren't that great.  We should mention here that this is a much smaller model than most so we do not expect the best accuracy values for this network, but for the two foreground objects, the network does really really well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-TRT\n",
    "\n",
    "As mentioned before, there is a third option, TensorFlow TensorRT integration.\n",
    "\n",
    "The reasons for integrating TensorRT into native TensorFlow are two-fold:\n",
    "\n",
    "- If you are already developing a model in TensorFlow, it makes sense in some cases to stay in that workflow when moving to inference.\n",
    "- If a model is not able to be fully converted by TensorRT, the TensorFlow-TensorRT integration will convert portions of the model it is able to convert and leave others as TensorFlow subgraphs.\n",
    "\n",
    "For these reasons, NVIDIA has worked with TensorFlow teams to integrate TensorRT functionality into TensorFlow. Although you may not see the exact same performance improvements that you would with native TensorRT, TF-TRT provides a great alternative with relatively simple code additions.\n",
    "\n",
    "For TensorFlow <= 1.13 the conversion process looks something like this:\n",
    "\n",
    "```python\n",
    "import tensorflow.contrib.tensorrt as trt\n",
    "# For a frozen graph\n",
    "convert_graph_def = trt.create_inference_graph(\n",
    "    input_graph_def=frozen_graph,\n",
    "    outputs=['logits', 'classes'],\n",
    "    precision_mode=\"FP16\")\n",
    "# For a saved model\n",
    "trt.create_inference_graph(\n",
    "    input_saved_model_dir=input_saved_model_dir,\n",
    "    output_saved_model_dir=output_saved_model_dir,\n",
    "    precision_model=\"FP16\")\n",
    "```\n",
    "\n",
    "For TensorFlow 1.14+ (1.x versions > 1.13), we will use something slightly different:\n",
    "\n",
    "```python\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "converter = trt.TrtGraphConverter(\n",
    "    input_saved_model_dir=input_saved_model_dir,\n",
    "    precision_model=trt.TrtPrecisionMode.FP16)\n",
    "converter.convert()\n",
    "converter.save(output_saved_model_dir)\n",
    "```\n",
    "\n",
    "And for TensorFlow 2.x, we extend this a little bit differently:\n",
    "\n",
    "```python\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "conversion_params = trt.TRTConversionParams(\n",
    "    precision_model=trt.TrtPrecisionMode.FP16)\n",
    "converter = trt.TrtGraphConverterV2(\n",
    "    input_saved_model_dir=input_saved_model_dir,\n",
    "    conversion_params=conversion_params)\n",
    "converter.convert()\n",
    "# optionally build TRT engines before deployment\n",
    "converter.build(input_fun=my_input_fn)\n",
    "converter.save(output_saved_model_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert\n",
    "\n",
    "def create_tftrt_model():\n",
    "    converter = trt_convert.TrtGraphConverter(input_saved_model_dir = SAVED_MODEL_PATH,\n",
    "                                              precision_mode='FP16'\n",
    "                                             )\n",
    "    converter.convert()\n",
    "    converter.save(TFTRT_MODEL_PATH)\n",
    "\n",
    "create_tftrt_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "def run_tftrt_sess():\n",
    "    times = []\n",
    "    \n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        tf.saved_model.loader.load(sess,\n",
    "                                   [tf.saved_model.tag_constants.SERVING],\n",
    "                                   TFTRT_MODEL_PATH)\n",
    "        image_tensor = sess.graph.get_tensor_by_name('image_tensor:0')\n",
    "        boxes = sess.graph.get_tensor_by_name('detection_boxes:0')\n",
    "        scores = sess.graph.get_tensor_by_name('detection_scores:0')\n",
    "        classes = sess.graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = sess.graph.get_tensor_by_name('num_detections:0')\n",
    "        \n",
    "        iterations = 10\n",
    "        \n",
    "        input_image = load_img(INPUT_IMG, 'tf')\n",
    "        for index in range((iterations)):\n",
    "            start = time.time()\n",
    "            sess.run([boxes, scores, classes, num_detections], feed_dict={image_tensor: input_image})\n",
    "            stop = time.time()\n",
    "            if index > 1:\n",
    "                times.append(stop-start)\n",
    "                print(\"Time per run for TensorFlow Inference %d: %f ms\" % (index, (stop - start) * 1000))\n",
    "                \n",
    "run_tftrt_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/6/6d/Nvidia_image_logo.svg/1200px-Nvidia_image_logo.svg.png\" width=\"250\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
