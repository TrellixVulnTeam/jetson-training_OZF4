{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/6/6d/Nvidia_image_logo.svg/1200px-Nvidia_image_logo.svg.png\" width=\"250\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2020 NVIDIA\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepStream\n",
    "\n",
    "In this notebook, we will introduce NVIDIA's DeepStream SDK and how it can be used for video analytics applications.\n",
    "\n",
    "## Background\n",
    "\n",
    "Deepstream applications introduce deep neural networks and other complex processing tasks into a stream processing pipeline to enable near real-time analytics on video and other sensor data.  Extracting meaningful insights from these sensors creates opportunities for improved operational efficiences and safety.  Cameras, for example, are the most deployed IoT sensor currently in use; cameras are foudn in our homes, on our streets, in parking los, shopping malls, warehouses, factories - they are everywhere.  The potential use of video analytics is enormous: access control, loss prevention, automated checkout, surveillance, safety, automated inspection (QA), package sort (smart logistics), traffic control/engineering, industrial automation, etc.\n",
    "\n",
    "<center><img src=\"https://developer.nvidia.com/sites/default/files/akamai/ds_new.jpg\" width=\"750\"></center>\n",
    "<center>Image credit: https://developer.nvidia.com/sites/default/files/akamai/ds_new.jpg</center>\n",
    "\n",
    "Although intelligent video analytics (IVA) differs by industry and aplication, the flow from pixels to insights remains consistent across all use cases.  It is this common workflow that is the foundation for the DeepStream SDK generic streaming analytics plug and play architecture.\n",
    "\n",
    "<center><img src=\"https://developer.nvidia.com/sites/default/files/akamai/DS_EdgetoCloud_GA_Productpage_Cropped.jpg\" width=\"750\"></center>\n",
    "<center>Image credit: https://developer.nvidia.com/sites/default/files/akamai/DS_EdgetoCloud_GA_Productpage_Cropped.jpg</center>\n",
    "\n",
    "More specifically, a DeepStream application is a set of modular plugins connected to form a processing pipeline.  Each plugin represents a functional block, e.g., inference using TensorRT, or multi-stream decode.  Hardware accelerated plugins interact with the underlying hardware (where applicable) to deliver maximum performance.  For example, the decode plugin interacts with NVDEC, and then inference plugin interacts with the GPU or DLA.  Each plugin can be instantiated multiple times within a pipeline as needed.\n",
    "\n",
    "<center><img src=\"https://developer.nvidia.com/sites/default/files/akamai/ds-3-workflow%20%281%29.jpg\" width=\"750\"></center>\n",
    "<center>Image credit: https://developer.nvidia.com/sites/default/files/akamai/ds-3-workflow%20%281%29.jpg</center>\n",
    "\n",
    "\n",
    "Just a quick overview of a few of the accelerators located in our Jetson products:\n",
    "\n",
    "- DLA - Deep Learning Accelerator (Jetson AGX Xavier only)\n",
    "- PVA - Programmable Vision Accelerator (Jetson AGX Xavier only)\n",
    "- ISP - Image Signal Processing\n",
    "- VIC - Vision Image Compositor\n",
    "\n",
    "## Deepstream SDK\n",
    "\n",
    "The [NVIDIA DeepStream SDK](https://developer.nvidia.com/deepstream-sdk) is a streaming analytics toolkit based on the open source GStreamer multimedia framework.  The DeepStream SDK accelerates development of scalable IVA applicatins, making it easier for developers to build core deep learning networks instead of designing end-to-end applications from scratch.  The SDK is supported on systems that contain an NVIDIA Jetson module or an NVIDA dGPU adapter; it is comprisde of an extensible collection of hardware-accelerated plugins that interact with low-level libraries to optimize performance and defines a standardized metadata structure enabling custom/user-specific additions.\n",
    "\n",
    "The materials and descriptions in this notebook provide a quick overview, for more detailed information and explanation of the DeepStream SDK refer to the following materials:\n",
    "\n",
    "- [NVIDIA DeepStream SDK Development Guide](https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html)\n",
    "- [NVIDIA DeepStream Plugin Manual](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/index.html)\n",
    "- [NVIDIA DeepStream SDK API Reference Documentation](https://docs.nvidia.com/metropolis/deepstream/dev-guide/DeepStream%20Development%20Guide/baggage/index.html)\n",
    "\n",
    "### Reference Applications\n",
    "\n",
    "The DeepStream SDK is packaged with several test applications including pre-trained models, example configuration files and sample video streams that can be used to run those applications. Additional samples and source code examples provide enough information to jumpstart development efforts for most IVA use cases. Test applications demonstrate:\n",
    "\n",
    "- How to use DeepStream elements (e.g., get source, decode and mux multiple streams, run inference on pre-trained models, annotate and render image)\n",
    "- How to generate a batch of frames and run inference on it for better resource utilization\n",
    "- How to add custom/user-specific metadata to any component of DeepStream\n",
    "- And much more... see the NVIDIA DeepStream SDK Development Guide for complete details\n",
    "\n",
    "### GStreamer Plugins\n",
    "\n",
    "GStreamer is a framework for plugins, data flow, and media type handling/negotiation. It is used to create streaming media applications. Plugins are shared libraries that are dynamically loaded at runtime and can be extended and upgraded independently. When arranged and linked together, plugins form the processing pipeline that defines the data flow for a streaming media application. You can learn more about GStreamer through its extensive online documentation, beginning with [\"What is GStreamer?\"](https://gstreamer.freedesktop.org/documentation/application-development/introduction/gstreamer.html?gi-language=c).\n",
    "\n",
    "In addition to the open source plugins you'll find in the GStreamer framework libraries, the DeepStream SDK includes NVIDIA hardware accelerated plugins that leverage GPU capabilities. For a complete list of DeepStream GStreamer plugins, see the [NVIDIA DeepStream Plugin Manual](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/index.html#page/DeepStream_Plugin_Manual%2Fdeepstream_plugin_introduction.html).\n",
    "\n",
    "Some of the noteable NVIDIA Hardware Accelerated plugins include:\n",
    "- [Gst-nvstreammux](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/DeepStream_Plugin_Manual/deepstream_plugin_details.02.03.html) - Batch streams before sending for AI inference.\n",
    "- [Gst-nvinfer](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/DeepStream_Plugin_Manual/deepstream_plugin_details.02.01.html#wwpID0E0IZ0HA) - Run inference using TensorRT.\n",
    "- [Gst-nvvideo4linux2](https://docs.nvidia.com/metropolis/deepstream/4.0.1/plugin-manual/index.html#page/DeepStream_Plugin_Manual/deepstream_plugin_details.02.12.html) - Decode video streams using the hardware accelerated decoder (NVDEC); Encode RAW data in I420 format to H264 or H265 output video stream using hardware accelerated encoder (NVENC).\n",
    "- [Gst-nvvideoconvert](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/DeepStream_Plugin_Manual/deepstream_plugin_details.02.07.html) - Perform video color format conversion. The first Gst-nvvideoconvert plugin before Gst-nvdsosd plugin converts stream data from I420 to RGBA and the second Gst-nvvideoconvert plugin after Gst-nvdsosd plugin converts data from RGBA to I420.\n",
    "- [Gst-nvdsosd](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/DeepStream_Plugin_Manual/deepstream_plugin_details.02.06.html) - Draw bounding boxes, text, and region of interest (ROI) polygons.\n",
    "- [Gst-nvtracker](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/DeepStream_Plugin_Manual/deepstream_plugin_details.02.02.html) - Track object between frames.\n",
    "- [Gst-nvmultistreamtiler](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/index.html#page/DeepStream_Plugin_Manual%2Fdeepstream_plugin_details.02.05.html) - Composite a 2D tile from batched buffers.\n",
    "- [Gst-nvv4l2decoder](https://developer.download.nvidia.com/embedded/L4T/r32_Release_v1.0/Docs/Accelerated_GStreamer_User_Guide.pdf) - Decode a video stream.\n",
    "- [Gst-Nvv4l2h264enc](https://developer.download.nvidia.com/embedded/L4T/r32_Release_v1.0/Docs/Accelerated_GStreamer_User_Guide.pdf) - Encode a video stream.\n",
    "- [Gst-NvArgusCameraSrc](https://developer.download.nvidia.com/embedded/L4T/r32_Release_v1.0/Docs/Accelerated_GStreamer_User_Guide.pdf) - Provide options to control ISP properties using the Argus API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing here where we make sure we are running at highest clock frequency so we get the most performance out of our device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo nvidia | sudo -S nvpmodel -m 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo nvidia | sudo -S nvpmodel -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo nvidia | sudo -S jetson_clocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepStream Configuration Files\n",
    "\n",
    "One of the ways to interact with DeepStream is to create/edit a configuration tile which tells the SDK how to construct the pipeline.\n",
    "\n",
    "The Deepstream SDK comes with multiple sample configuration files that can be used out of the box for different use cases.  Some of these include (these are located in the `/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/` directory on your Jetson device where DeepStream was installed:\n",
    "\n",
    "- **config_infer_primary.txt:** Configures a nvinfer element as primary detector.\n",
    "- **config_infer_secondary_carcolor.txt**, **config_infer_secondary_carmake.txt**, **config_infer_secondary_vehicletypes.txt:** Configure a nvinfer element as secondary classifier.\n",
    "- **iou_config.txt:** Configures a low-level IOU (Intersection over Union) tracker.\n",
    "- **tracker_config.yml:** Configures the NvDCF tracker.\n",
    "- **source1_usb_dec_infer_resnet_int8.txt:** Demonstrates one USB camera as input.\n",
    "- **source1_csi_dec_infer_resnet_int8.txt:** Demonstrates one CSI camera as input; for Jetson only.\n",
    "- **source2_csi_usb_dec_infer_resnet_int8.txt:** Demonstrates one CSI camera and one USB camera as inputs; for Jetson only.\n",
    "- **source6_csi_dec_infer_resnet_int8.txt:** Demonstrates six CSI cameras as inputs; for Jetson only.\n",
    "- **source8_1080p_dec_infer-resnet_tracker_tiled_display_fp16_nano.txt:** Demonstrates 8 Decode + Infer + Tracker; for Jetson Nano only.\n",
    "\n",
    "We have created a few specialty configuration files based on these default ones specifically for this workshop.  The first configuration file we want to look at is the [source2.txt](deepstream-configs/source2.txt).  Feel free to look at the entire file, but a few key pieces are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's investigate the `source` portion of the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n 28,39p deepstream-configs/source2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice with the `source` portion of the configuration file (basically the pipeline input), we can set things like the type of video source (cameras, URI, RTSP, etc.) as well as memory types to use during the pipeline and GPU_ID (on the Jetson Nano, we only have 1 GPU, but on other machines, you could potentially have more).  For this example, notice that we are using MultiURI, but only providing 1 URI and then specifying `num-sources=2`.  This will just replicate the same video twice (in essence, streaming 2 videos, they just have the same content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next if we look at the `sink` portion of the configuration file, we can see a few options for streaming the output (i.e. output file, RTSP, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n 41,83p deepstream-configs/source2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a few different `sink` types.  First is the `EglSink` which will act as a on on screen display in most cases.  `sink1` represents output of the pipeline being streamed to a file (in this case an .mp4 file).  Notice with this option you can specify which codec to use (h264 or h265) as well as wehter to use the hardware encoder as part of the device or a software encoder for the video stream.  The last sink, `sink2`, represents output to an RTSP stream.  Once running, this can be viewed with most video streaming players (like VLC).  To access, you can simply navigate to `rtsp://<ip-address>:8554/ds-test` and the browser will ask to open VLC (or equivalent on your local machine).\n",
    "\n",
    "Notice that only one of these is enabled at this point (for this particular configuration, we will be streaming directly to the on-screen display as an EglSink).  For the purposes of this lab, however, we will be using the second option (output to a file, mp4 video) so that it is viewable in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few other configuration parameters that we could modify include `tiled-display` for creating the output view in tiled format, `osd` for on-screen display settings, and `streammux` which handles the multiplexing of the multiple streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this configuration, you can also set paths and properties of the models you wish to use for inference.  For example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n 122,180p deepstream-configs/source2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see each of the properties designated with a `gie` represent one of the models that will be used in the inference pipeline.  In this case, we will be running a primary object detection network (i.e. `primary-gie`) which can detect vehicles and people.  We then pass that information on to a couple of different classification models (i.e. `secondary-gie[0,1,2]`) which are for classifying vehicle type, vehicle color, and vehicle make, respectively.\n",
    "\n",
    "Notice there is a also a `tracker` here (which is not enabled) that could be used for giving each instance of vehicle or person in the video it's own unique ID.\n",
    "\n",
    "Lastly, notice for each of the `gie` sections, we provide a `config-file`.  Inside these config files, you can find all of the components which correspond to the model itself including model path, batch size, classes, custom parsers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Try It\n",
    "\n",
    "Now that we understand a little bit about the DeepStream configuration files which will help build the pipeline, we can now see a few examples of what the output looks like.  Note again that for this demonstration we will be creating an mp4 as output instead of on-screen display since we are inside of a Jupyter notebook, but the config files can be modified to use any of the `sink` options at a later time.\n",
    "\n",
    "First, let's copy our configuration files to the location where Deepstream was installed so that it's easier for it to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo nvidia | sudo -S cp deepstream-configs/* /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to run the DeepStream SDK we can simply run `deepstream-app` with the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepstream-app --help-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have put most of our configuration inside the config files earlier, we can simply use those instead of command line arguments in order to successfully run the `deepstream-app`.\n",
    "\n",
    "#### 2 Streams\n",
    "\n",
    "Let's see what happens when we run 2 streams (notice if you look at this [config file](deepstream-configs/source2_mp4.txt), you will see that we are using a primary detector as well as 3 secondary classifiers).\n",
    "\n",
    "You'll notice that when you first run the `deepstream-app` that there is an error.  This is alright, the error is just pointing out the the TensorRT engine does not exist and that is needs to build it.  Also you will notice if you read through some of the output that INT8 is not supported by this platform (namely, Jetson Nano); so eventhough we set INT8 mode in our configuration file, the app will automatically try to create the next-best-thing (i.e. FP16) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unset DISPLAY && deepstream-app -c /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/source2_mp4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even with all 4 of those deep learning networks (1 detection and 3 classification), we are still able to achieve almost 30fps on 2 video streams.\n",
    "\n",
    "Let's check out the output that we made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video width=\"960\" height=\"720\" controls>\n",
    "        <source src=\"out_2streams.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 Streams\n",
    "\n",
    "Now, let's try something a little bigger.  Now let's try to remove the secondary classifiers but increase the number of streams we are processing at once.  For this example, we will try 8 streams with the object detection model with the addition of the tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unset DISPLAY && deepstream-app -c /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/source8_mp4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice for this one, we are actually sustaining 30FPS for each of the 8 streams across the entire length of the video.  This means that we are able to run that detection model and a KLT tracker on 8 streams of video at once and still achieve the full 30FPS that the video is being streamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video width=\"960\" height=\"720\" controls>\n",
    "        <source src=\"out_8streams.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python API with GStreamer\n",
    "\n",
    "Up until now, to create the sample applications, we have been using the configuration files.  However, another way that you can use the DeepStream SDK is through the python API where you instantiate GStreamer plugins for each of the portions of your pipeline and then put them all together to form the final pipeline.\n",
    "\n",
    "We will not cover this in this particular lab, but for more information, some of our Python samples can be found either in our DeepStream docker containers([dGPU](https://ngc.nvidia.com/catalog/containers/nvidia:deepstream), [Jetson](https://ngc.nvidia.com/catalog/containers/nvidia:deepstream-l4t)) on [NGC](http://ngc.nvidia.com) or on [Github](https://github.com/NVIDIA-AI-IOT/deepstream_python_apps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As just a quick sneak peak, here's what it would look like to build a very simple pipeline using the Python API.\n",
    "\n",
    "Creating a source element for reading from a file\n",
    "```python\n",
    "source = Gst.ElementFactory.make(\"filesrc\", \"file-source\")\n",
    "if not source:\n",
    "    sys.stderr.write(\" Unable to create Source \\n\")\n",
    "```\n",
    "\n",
    "Creating a h264 parser\n",
    "```python\n",
    "print(\"Creating H264Parser \\n\")\n",
    "h264parser = Gst.ElementFactory.make(\"h264parse\", \"h264-parser\")\n",
    "if not h264parser:\n",
    "    sys.stderr.write(\" Unable to create h264 parser \\n\")\n",
    "```\n",
    "\n",
    "Using nvdec_h264 for hardware accelerated decoding on GPU\n",
    "```python\n",
    "print(\"Creating Decoder \\n\")\n",
    "decoder = Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder\")\n",
    "if not decoder:\n",
    "    sys.stderr.write(\" Unable to create Nvv4l2 Decoder \\n\")\n",
    "```\n",
    "\n",
    "Create a nvstreammux instance to form batches for one or more sources\n",
    "```python\n",
    "streammux = Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\")\n",
    "if not streammux:\n",
    "    sys.stderr.write(\" Unable to create NvStreamMux \\n\")\n",
    "```\n",
    "\n",
    "Setting up nvinfer to run inference on decoders output\n",
    "```python\n",
    "pgie = Gst.ElementFactory.make(\"nvinfer\", \"primary-inference\")\n",
    "if not pgie:\n",
    "    sys.stderr.write(\" Unable to create pgie \\n\")\n",
    "```\n",
    "\n",
    "Use a converter to convert from NV12 to RGBA as required by nvosd\n",
    "```python\n",
    "nvvidconv = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor\")\n",
    "if not nvvidconv:\n",
    "    sys.stderr.write(\" Unable to create nvvidconv \\n\")\n",
    "```\n",
    "\n",
    "Create OSD to draw on the converted RGBA buffer\n",
    "```python\n",
    "nvosd = Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\")\n",
    "if not nvosd:\n",
    "    sys.stderr.write(\" Unable to create nvosd \\n\")\n",
    "```\n",
    "\n",
    "Finally render the osd output\n",
    "```python\n",
    "print(\"Creating EGLSink \\n\")\n",
    "sink = Gst.ElementFactory.make(\"nveglglessink\", \"nvvideo-renderer\")\n",
    "if not sink:\n",
    "    sys.stderr.write(\" Unable to create egl sink \\n\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/6/6d/Nvidia_image_logo.svg/1200px-Nvidia_image_logo.svg.png\" width=\"250\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
