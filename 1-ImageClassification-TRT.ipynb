{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/6/6d/Nvidia_image_logo.svg/1200px-Nvidia_image_logo.svg.png\" width=\"250\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2020 NVIDIA\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification w/ TensorRT\n",
    "\n",
    "This notebook will illustrate the full pipeline of creating a very simple image classification model with TensorFlow/Keras, converting that to UFF, and then using the UFF file to create a TensorRT engine and run inference.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting MaxN mode\n",
    "\n",
    "First, before we do anything else, we want to make sure that we put the Jetson Nano in MaxN mode.  This will mean that the the clock frequencies will be set to their highest in order to achieve the lowest inference times.\n",
    "\n",
    "To do this, we want to set the max/min frequency to a preferred value, which we can do with the `nvpmodel` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo nvidia | sudo -S nvpmodel -m 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo nvidia | sudo -S nvpmodel -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to use the `jetson_clocks` script to fix the frequency to maximal.  This will allow us to get the most performance out of our device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo nvidia | sudo -S jetson_clocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Now that we have out clock frequencies set, let's go ahead and import all of that packages that we will be using for this particular notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorrt as trt\n",
    "import uff\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], \"..\"))\n",
    "import common\n",
    "\n",
    "print('TensorFlow version: ', tf.__version__)\n",
    "print('TensorRT version: ', trt.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the necessary packages, now let's create a model and convert that to TensorRT.  First we want to create a couple of helper functions that will allow us to grab the data, create the LeNet5 model, perform the training, and then save the output model.\n",
    "\n",
    "The dataset we will train on for this small model is the MNIST dataset consisting of handwritten digits in the form of 28x28 images.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/530/1*VAjYygFUinnygIx9eVCrQQ.png\" alt=\"MNIST Dataset\" width=\"750\"/></center>\n",
    "<center>Image credit: https://miro.medium.com/max/530/1*VAjYygFUinnygIx9eVCrQQ.png<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset():\n",
    "    # Import the data\n",
    "    (x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train_orig, y_train_orig, x_test_orig, y_test_orig = x_train, y_train, x_test, y_test\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    # Reshape the data\n",
    "    NUM_TRAIN = 60000\n",
    "    NUM_TEST = 10000\n",
    "    x_train = np.reshape(x_train, (NUM_TRAIN, 28, 28, 1))\n",
    "    x_test = np.reshape(x_test, (NUM_TEST, 28, 28, 1))\n",
    "    return x_train, y_train, x_test, y_test, x_train_orig, y_train_orig "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that we will be creating is a simple version of the LeNet model shown in the image below containing Flatten and Dense (fully-connected) layers.\n",
    "\n",
    "<center><img src=\"https://engmrk.com/wp-content/uploads/2018/09/LeNet_Original_Image.jpg\" alt=\"LeNet5 Architecture\" width=\"750\"/></center>\n",
    "<center>Image credit: https://engmrk.com/wp-content/uploads/2018/09/LeNet_Original_Image.jpg<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[28,28, 1], name=\"input\"))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax, name=\"output\"))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, filename):\n",
    "    # First freeze the graph and remove training nodes.\n",
    "    output_names = model.output.op.name\n",
    "    sess = tf.keras.backend.get_session()\n",
    "    frozen_graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), [output_names])\n",
    "    frozen_graph = tf.graph_util.remove_training_nodes(frozen_graph)\n",
    "    # Save the model\n",
    "    with open(filename, \"wb\") as ofile:\n",
    "        ofile.write(frozen_graph.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our helper functions, let's go ahead and train our model.  Note that this should run relatively quickly since we are using a small model and a very small dataset.\n",
    "\n",
    "**NOTE:** Normally you wouldn't do training on a device like the Jetson, you would perform training on a more capable GPU; for example, an A100 or V100 in AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and preprocess the MNIST dataset\n",
    "x_train, y_train, x_test, y_test, x_train_orig, y_train_orig = process_dataset()\n",
    "\n",
    "tf_frozen_model_file = \"models/lenet5.pb\"\n",
    "tf_saved_model_path = \"models/lenet5_saved_model_tf\"\n",
    "\n",
    "if not os.path.exists(os.path.dirname(tf_frozen_model_file)):\n",
    "    os.mkdir(os.path.dirname(tf_frozen_model_file))\n",
    "\n",
    "# Create the LeNet5 model (using the tf.keras API)\n",
    "model = create_model()\n",
    "\n",
    "# Train the model on the data\n",
    "history = model.fit(x_train, y_train, epochs = 2, verbose = 1, validation_data = (x_test, y_test))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss_and_metrics = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss\", loss_and_metrics[0])\n",
    "print(\"Test Accuracy\", loss_and_metrics[1])\n",
    "\n",
    "# Save the model as a frozen graph (for use with UFF/TRT)\n",
    "save(model, filename=tf_frozen_model_file)\n",
    "\n",
    "# Save the model as a TF saved model (for use with TF-TRT)\n",
    "model.save(tf_saved_model_path, save_format=\"tf\")\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(x_train_orig[i], cmap='gray', interpolation='none')\n",
    "    plt.title('Digit: {}'.format(y_train_orig[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = model.predict_classes(x_test)\n",
    "\n",
    "# see which we predicted correctly and which not\n",
    "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]\n",
    "print(len(correct_indices),\" classified correctly\")\n",
    "print(len(incorrect_indices),\" classified incorrectly\")\n",
    "\n",
    "# adapt figure size to accomodate 18 subplots\n",
    "plt.rcParams['figure.figsize'] = (7,14)\n",
    "\n",
    "figure_evaluation = plt.figure()\n",
    "\n",
    "# plot 9 correct predictions\n",
    "for i, correct in enumerate(correct_indices[:9]):\n",
    "    plt.subplot(6,3,i+1)\n",
    "    plt.imshow(x_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\n",
    "      \"Predicted: {}, Truth: {}\".format(predicted_classes[correct],\n",
    "                                        y_test[correct]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "# plot 9 incorrect predictions\n",
    "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
    "    plt.subplot(6,3,i+10)\n",
    "    plt.imshow(x_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\n",
    "      \"Predicted {}, Truth: {}\".format(predicted_classes[incorrect], \n",
    "                                       y_test[incorrect]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "figure_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we are saving the model twice here.  We will be using the frozen graph format (i.e. the .pb file) to create out UFF file (and consequently, our TensorRT engine), but we could go a different route and use a TensorFlow saved model to create a TensorFlow-TensorRT model.  More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFF Conversion\n",
    "\n",
    "Now that we have our trained model, we can take that trained model and convert it to the UFF file format.  The reason we are doing this are two-fold:\n",
    "\n",
    "* Easier to convert to a TensorRT model\n",
    "* Intermediate file formats (like UFF/ONNX) provide much more flexibility when going between different frameworks (i.e. TensorFlow, PyTorch ,TensorRT, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a class that will house the information for our model and then read in our TensorFlow frozen graph so that we can use it for the conversion.\n",
    "\n",
    "We want to get the proper information about our model, for that we will use the `saved_model_cli_show` function that will give us information about inputs/outputs/shapes/etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --all --dir \"models/lenet5_saved_model_tf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the input and output nodes here are conveniently named `input` and `output` for simplicity, but in general it is always good to know the information about your model just incase these have changed.  If you don't have the correct information, it will cause problems later on down the road when we try to convert the model.\n",
    "\n",
    "**NOTE:** Even though we set our output node to be `output`, tf.keras has appended Softmax to the name here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelData(object):\n",
    "    UFF_MODEL_NAME = \"models/lenet5.uff\"\n",
    "    PB_MODEL_NAME = \"models/lenet5.pb\"\n",
    "    INPUT_NAME =\"input\"\n",
    "    INPUT_SHAPE = (1, 28, 28)\n",
    "    OUTPUT_NAME = \"output/Softmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    output_graph_def = tf.GraphDef()\n",
    "    with open(ModelData.PB_MODEL_NAME, \"rb\") as f:\n",
    "        output_graph_def.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model has been read in and parsed into `output_graph_def` so we can go ahead and use the `uff.from_tensorflow` operator to convert this to a UFF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_to_uff(graphdef):\n",
    "    uff_model = uff.from_tensorflow(\n",
    "        graphdef=graphdef,\n",
    "        output_filename=ModelData.UFF_MODEL_NAME,\n",
    "        output_nodes=[ModelData.OUTPUT_NAME],\n",
    "        text=False)\n",
    "    \n",
    "model_to_uff(output_graph_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT Engine Creation\n",
    "\n",
    "We have our intermediate file now (in the form of UFF), so now we can go through the process of creating a TensorRT engine from that UFF file.\n",
    "\n",
    "First, we want to set the logger for TensorRT to suppress messages that we don't need to see (i.e. warnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set the logger severity higher to suppress messages (or lower to display more messages).\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, since we are running on a Jetson Nano, the process of building a TensorRT engine can sometimes take a while depending on the size of the network.  For this particular example, since we are using such a small model, it should only take a couple of seconds.\n",
    "\n",
    "But what exactly is going on when you build a TensorRT engine?\n",
    "\n",
    "* TensorRT has the ability to convert models from all the major frameworks and deploy them on any NVIDIA products (enterprise GPUs all the way to Jetson devices)\n",
    "* TensorRT does layer fusion which combines layers horizontally and vertically so the computation is able to be done in a single CUDA kernel\n",
    "* TensorRT performs precision calibration and converts layers and parameters to FP16 (or even INT8) to make operations faster\n",
    "* TensorRT finds the correct kernel that will perform the best on the given hardware (different kernels for the same function on different hardware)\n",
    "\n",
    "With all of that in mind, let's go ahead and create the function that will build our TensorRT engine.  Notice that we are using the `trt.Builder()` and `trt.Builder.create_network()` operators to aide in our creation of the engine.  We are also using `trt.UffParser` which alleviates some of the headache in converting a UFF file to a TensorRT engine.\n",
    "\n",
    "**NOTE:** There is an equivalent parser for ONNX (`trt.ONNXParser`) if you were to ever need that pathway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_engine(model_file):\n",
    "    # For more information on TRT basics, refer to the introductory samples.\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network() as network, trt.UffParser() as parser:\n",
    "        # Set the workspace size for kernels selection\n",
    "        builder.max_workspace_size = common.GiB(1)\n",
    "        \n",
    "        # Parse the Uff Network\n",
    "        parser.register_input(ModelData.INPUT_NAME, ModelData.INPUT_SHAPE)\n",
    "        parser.register_output(ModelData.OUTPUT_NAME)\n",
    "        parser.parse(model_file, network)\n",
    "        \n",
    "        # Build and return an engine (underneath creating a CUDA context for creation).\n",
    "        return builder.build_cuda_engine(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a simple helper function that will choose a random image from out dataset for us to run inference on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a test case into the provided pagelocked_buffer.\n",
    "def load_normalized_test_case(data_paths, pagelocked_buffer, case_num=randint(0, 9)):\n",
    "    [test_case_path] = common.locate_files(data_paths, [str(case_num) + \".pgm\"])\n",
    "    # Flatten the image into a 1D array, normalize, and copy to pagelocked memory.\n",
    "    img = np.array(Image.open(test_case_path)).ravel()\n",
    "    np.copyto(pagelocked_buffer, 1.0 - img / 255.0)\n",
    "    return case_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our helper function, we can go ahead and use our `build_engine` function to create out TensorRT engine, serialize (save) it, and run inference on it to make sure we see the result we should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths, _ = common.find_sample_data(description=\"Runs an MNIST network using a UFF model file\", subfolder=\"mnist\")\n",
    "\n",
    "with build_engine(ModelData.UFF_MODEL_NAME) as engine:\n",
    "    # Build an engine, allocate buffers and create a stream.\n",
    "    # For more information on buffer allocation, refer to the introductory samples.\n",
    "    inputs, outputs, bindings, stream = common.allocate_buffers(engine)\n",
    "    with engine.create_execution_context() as context:\n",
    "        case_num = load_normalized_test_case(data_paths, pagelocked_buffer=inputs[0].host)\n",
    "        # For more information on performing inference, refer to the introductory samples.\n",
    "        # The common.do_inference function will return a list of outputs - we only have one in this case.\n",
    "        start = time.time()\n",
    "        [output] = common.do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "        stop = time.time()\n",
    "        pred = np.argmax(output)\n",
    "        print(\"Test Case: \" + str(case_num))\n",
    "        print(\"Prediction: \" + str(pred))\n",
    "        print(\"Time for 15 TensorRT Inferences with memcpys, etc.: %f ms\" % ((stop - start)*1000))\n",
    "        with open(\"models/lenet5.engine\", \"wb\") as f:\n",
    "            f.write(engine.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now gone through the process of taking a very simple model and converting it into a TensorRT engine and using it for inference.\n",
    "\n",
    "Now let's tackle something a little bit more difficult (as well as useable).  Continue onto [2-ObjectDetection-TRT.ipynb](2-ObjectDetection-TRT.ipynb) to go through the same process we have just gone through, but this time for an object detection model (SSD).  We will also discuss a little more in-depth about what is going on since it is a slightly more complex network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/6/6d/Nvidia_image_logo.svg/1200px-Nvidia_image_logo.svg.png\" width=\"250\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
